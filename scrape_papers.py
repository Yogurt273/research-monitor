import json
import feedparser             # pip install feedparser
import pandas as pd
from pathlib import Path

# è¾“å‡ºç›®å½•
OUT = Path("data"); OUT.mkdir(exist_ok=True)

# arXiv APIï¼šæŸ¥è¯¢ cs.AI ï¼ˆäººå·¥æ™ºèƒ½ï¼‰åˆ†ç±»ï¼Œæœ€æ–° 200 ç¯‡
API_URL = (
    "http://export.arxiv.org/api/query?"
    "search_query=cat:cs.AI&"
    "start=0&max_results=200&"
    "sortBy=submittedDate&sortOrder=descending"
)

def crawl():
    feed = feedparser.parse(API_URL)
    rows = []
    for e in feed.entries:
        date = e.published[:10]         # å– YYYY-MM-DD
        title = e.title.strip().replace("\n"," ")
        link  = e.link
        rows.append({"date": date, "title": title, "link": link})
    print(f"âœ…  fetched {len(rows)} entries")
    return rows

def to_monthly(rows):
    df = pd.DataFrame(rows)
    df["month"] = pd.to_datetime(df["date"]).dt.to_period("M").astype(str)
    return df.groupby("month").size().to_dict()

if __name__ == "__main__":
    papers = crawl()
    # å†™å…¥æ–‡ä»¶
    Path("data/papers_raw.json").write_text(
        json.dumps(papers, ensure_ascii=False, indent=2)
    )
    Path("data/monthly_counts.json").write_text(
        json.dumps(to_monthly(papers), indent=2)
    )
    print("ğŸ‰  done")
